{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/tinyshakespeare/input.txt','r',encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in text 1115394\n"
     ]
    }
   ],
   "source": [
    "print('Total rows in text',len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars are: \n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
      "vocab_size is: 65\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary table\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'chars are: {''.join(chars)}')\n",
    "print(f'vocab_size is: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build tokenizer\n",
    "itos ={i:ch for i,ch in enumerate(chars)}\n",
    "stoi ={ch:i for i,ch in enumerate(chars)}\n",
    "\n",
    "encode = lambda s:[stoi[c] for c in s]\n",
    "decode = lambda l:''.join([itos[i] for i in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi there: hi there\n"
     ]
    }
   ],
   "source": [
    "print('hi there:',decode(encode('hi there')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape and dtype are: torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text),dtype=torch.long)\n",
    "print(f'data shape and dtype are: {data.shape} {data.dtype}')\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据分割成训练集和测试集\n",
    "n = int(len(data) * 0.9)\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 上下文长度\n",
    "block_size = 8\n",
    "# 9个训练数据实际包含了8组训练数据\n",
    "train_data[:block_size+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input sequence is tensor([18]), the target is 47\n",
      "when input sequence is tensor([18, 47]), the target is 56\n",
      "when input sequence is tensor([18, 47, 56]), the target is 57\n",
      "when input sequence is tensor([18, 47, 56, 57]), the target is 58\n",
      "when input sequence is tensor([18, 47, 56, 57, 58]), the target is 1\n",
      "when input sequence is tensor([18, 47, 56, 57, 58,  1]), the target is 15\n",
      "when input sequence is tensor([18, 47, 56, 57, 58,  1, 15]), the target is 47\n",
      "when input sequence is tensor([18, 47, 56, 57, 58,  1, 15, 47]), the target is 58\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f'when input sequence is {context}, the target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build batch，通过batch可以加速训练过程\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data)-block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x is:tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "y is:tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "------------------------------\n",
      "when input sequence is tensor([24]), the target is 43\n",
      "when input sequence is tensor([24, 43]), the target is 58\n",
      "when input sequence is tensor([24, 43, 58]), the target is 5\n",
      "when input sequence is tensor([24, 43, 58,  5]), the target is 57\n",
      "when input sequence is tensor([24, 43, 58,  5, 57]), the target is 1\n",
      "when input sequence is tensor([24, 43, 58,  5, 57,  1]), the target is 46\n",
      "when input sequence is tensor([24, 43, 58,  5, 57,  1, 46]), the target is 43\n",
      "when input sequence is tensor([24, 43, 58,  5, 57,  1, 46, 43]), the target is 39\n",
      "when input sequence is tensor([44]), the target is 53\n",
      "when input sequence is tensor([44, 53]), the target is 56\n",
      "when input sequence is tensor([44, 53, 56]), the target is 1\n",
      "when input sequence is tensor([44, 53, 56,  1]), the target is 58\n",
      "when input sequence is tensor([44, 53, 56,  1, 58]), the target is 46\n",
      "when input sequence is tensor([44, 53, 56,  1, 58, 46]), the target is 39\n",
      "when input sequence is tensor([44, 53, 56,  1, 58, 46, 39]), the target is 58\n",
      "when input sequence is tensor([44, 53, 56,  1, 58, 46, 39, 58]), the target is 1\n",
      "when input sequence is tensor([52]), the target is 58\n",
      "when input sequence is tensor([52, 58]), the target is 1\n",
      "when input sequence is tensor([52, 58,  1]), the target is 58\n",
      "when input sequence is tensor([52, 58,  1, 58]), the target is 46\n",
      "when input sequence is tensor([52, 58,  1, 58, 46]), the target is 39\n",
      "when input sequence is tensor([52, 58,  1, 58, 46, 39]), the target is 58\n",
      "when input sequence is tensor([52, 58,  1, 58, 46, 39, 58]), the target is 1\n",
      "when input sequence is tensor([52, 58,  1, 58, 46, 39, 58,  1]), the target is 46\n",
      "when input sequence is tensor([25]), the target is 17\n",
      "when input sequence is tensor([25, 17]), the target is 27\n",
      "when input sequence is tensor([25, 17, 27]), the target is 10\n",
      "when input sequence is tensor([25, 17, 27, 10]), the target is 0\n",
      "when input sequence is tensor([25, 17, 27, 10,  0]), the target is 21\n",
      "when input sequence is tensor([25, 17, 27, 10,  0, 21]), the target is 1\n",
      "when input sequence is tensor([25, 17, 27, 10,  0, 21,  1]), the target is 54\n",
      "when input sequence is tensor([25, 17, 27, 10,  0, 21,  1, 54]), the target is 39\n"
     ]
    }
   ],
   "source": [
    "xb,yb = get_batch('train')\n",
    "print(f'x is:{xb}')\n",
    "print(f'y is:{yb}')\n",
    "\n",
    "print('-'*30)\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b][:t+1]\n",
    "        target = yb[b][t]\n",
    "        print(f'when input sequence is {context}, the target is {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self,vocab_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size,vocab_size)\n",
    "        \n",
    "    def forward(self,idx,targets=None):\n",
    "        logits = self.token_embedding_table(idx)  #B,T,C    \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B,T,C = logits.shape\n",
    "            # 因为pytorch的cross_entropy函数要求输入(mini_batch,C)的格式，以bigram为例，C=vocab_size，实际有B*T个批次，所以需要将logits的形状变为(B*T,C)\n",
    "            logits = logits.view(B*T,C)\n",
    "            # targets也做相同的形状变换匹配输入格式\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits,targets)\n",
    "        \n",
    "        return logits,loss\n",
    "    \n",
    "    def generate(self,idx,max_new_tokens):\n",
    "        # idx是shape为（B,T）的输入，每个T时间窗内为token的index\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits,loss = self(idx)\n",
    "            # 取出最后一个时间步的输出作为输入的下一个token\n",
    "            logits = logits[:,-1,:] # becomes shape (B,C)\n",
    "            prob = F.softmax(logits,dim=-1) # becomes shape (B,C)\n",
    "            # 随机采样一个token\n",
    "            idx_next = torch.multinomial(prob,num_samples=1) # becomes shape (B,1)\n",
    "            # 增加到序列中，作为下一次输入\n",
    "            idx = torch.cat((idx,idx_next),dim=-1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size=vocab_size)\n",
    "logits,loss = m(xb,yb)\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4.8786, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 可以看到的loss是4.8786，但是我们可以预估初始的loss，应该是-ln(1/65)=4.1744。\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx = torch.zeros((1,1), dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "dRptRCPVjwW.RSVMjs-bgRkzrBTEa!!oP fRSxq.PLboTMkX'DUYepxIBFAYuxKXe.jeh\n",
      "sa!3MGFrSjuM:wX!?BTMl!.?,M:bQz\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=start_idx,max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pytorch optimizer\n",
    "optimizer = torch.optim.AdamW(m.parameters(),lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4940128326416016\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "for _ in range(10000):\n",
    "    xb,yb = get_batch(split='train')\n",
    "    \n",
    "    logits,loss = m(xb,yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Buru.\n",
      "Serth'Whth:\n",
      "AUESI isin. thed thmyo as ELI g.\n",
      "CHUSisthethellend:\n",
      "GAnd fo,\n",
      "CKI:\n",
      "DULird m wotiotoup-PAny, har lre uf,\n",
      "HEL:\n",
      "FLod bery s. angupr,\n",
      "\n",
      "MOWe t ld g, ts m reou ano f s thatheewisto burinoura s,\n",
      "Tono u t acou s, ingh cak r deftilselend wn. s acharofe sengor peey,\n",
      "Cod batood bllthither; i\n"
     ]
    }
   ],
   "source": [
    "# 因为这些token之间还没有互相交流，所以仅仅依靠bigram模型生成的结果较差。\n",
    "print(decode(m.generate(idx=start_idx,max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematic trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,2 # batch, time, channel\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里的主要想法是让token之间开始产生联系communication，产生联系的数字关系是求这些token在连续时间步上的平均值\n",
    "# 计算平均值是一种很弱的计算联系的方式，因为直观上来看会丢失大量的信息\n",
    "# 并且当前时间步的token的与其它token的关系计算方式是有讲究的：\n",
    "# 1.当前token只能与前面时间步的token产生联系，即第5个时间步的token只能与第1，2，3，4个时间步的token产生联系，因此第5个时间步的token存储的信息是前4个时间步的token加上第5个时间步自己的信息的平均值；\n",
    "# 2.第5个token不能与后续的token产生联系的原因是因为我们需要根据第5个时间步的token来预测第6个时间步的token，如果第6个时间步的token或者及后续的token也参与第5个token信息平均值的计算了，那么等于是泄露答案了；\n",
    "# 3.这种注意力机制的设计是为了让模型根据历史的信息来实现未来数据的预测。\n",
    "\n",
    "xbow = torch.zeros(B, T, C) # xbow代表的是词袋模型，通常代表数据的平均值\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b,:t+1] # t,C\n",
    "        xbow[b,t] = torch.mean(xprev, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.0894, -0.4926],\n",
       "        [ 0.1490, -0.3199],\n",
       "        [ 0.3504, -0.2238],\n",
       "        [ 0.3525,  0.0545],\n",
       "        [ 0.0688, -0.0396],\n",
       "        [ 0.0927, -0.0682],\n",
       "        [-0.0341,  0.1332]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里是聚合了历史token的当前token的信息值\n",
    "# 可以看到在第1个batch中，第1个时间步的token和xbow信息值都等于[ 0.1808, -0.0700]\n",
    "# 而在第2个时间步的token含有的信息[-0.0894, -0.4926]是通过计算，第1个时间步的token[ 0.1808, -0.0700]和第2个时间步的token[-0.3596, -0.9152]在T维度方向的平均值。\n",
    "xbow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1808, -0.0700],\n",
       "        [-0.3596, -0.9152],\n",
       "        [ 0.6258,  0.0255],\n",
       "        [ 0.9545,  0.0643],\n",
       "        [ 0.3612,  1.1679],\n",
       "        [-1.3499, -0.5102],\n",
       "        [ 0.2360, -0.2398],\n",
       "        [-0.9211,  1.5433]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 这里是原始token的值\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 8])\n",
      "torch.Size([4, 8, 2])\n",
      "tensor([[ 0.1808, -0.0700],\n",
      "        [-0.0894, -0.4926],\n",
      "        [ 0.1490, -0.3199],\n",
      "        [ 0.3504, -0.2238],\n",
      "        [ 0.3525,  0.0545],\n",
      "        [ 0.0688, -0.0396],\n",
      "        [ 0.0927, -0.0682],\n",
      "        [-0.0341,  0.1332]])\n"
     ]
    }
   ],
   "source": [
    "# version 2\n",
    "# 应用矩阵计算提高效率：\n",
    "# 设置function矩阵wei,用来计算每个时间步及其之前时间步的加权求和值\n",
    "wei = torch.tril(torch.ones(T, T)) # wei = weights 意为权重参数\n",
    "wei = wei/wei.sum(dim=1,keepdim=True)\n",
    "print(wei.shape) # 8,8\n",
    "xbow_new = wei @ x # 8,8 @ 4,8,2 -> 4,8,2\n",
    "print(xbow_new.shape) # 4,8,2\n",
    "print(xbow_new[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 检查两个tensor是否逐元素逐位置相等\n",
    "torch.allclose(xbow,xbow_new, rtol=1e-4, atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# version 3\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# 这里可以看成是初始各个时间步中的token之间的亲密度，初始为0，表示当前位置的token对其它时间步的token没有亲密度，\n",
    "# 但是随着训练，这些token会逐渐发现对其它某些时间步的token感兴趣，因此它们的值不会一直恒定是0\n",
    "wei = torch.zeros(T, T) \n",
    "# 通过下三角矩阵的mask遮蔽，保证了当前时间步的token只能从之前时间步的token中获取信息，而不能从之后的时间步获取信息\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "# 这里使用softmax将概率平均到每一个时间步的token上，使得每个时间步的token贡献权重都是相同的\n",
    "wei = wei.softmax(dim=1)\n",
    "xbow_3 = wei @ x # 8,8 @ 4,8,2 -> 4,8,2\n",
    "torch.allclose(xbow_new,xbow_3) \n",
    "\n",
    "# 这里就是self-attention机制的preview,利用下三角遮蔽矩阵对过去时间步的token进行加权聚合，mask遮蔽矩阵会告诉我们，\n",
    "# 当前时间步的token需要多少个历史token参与计算当前位置token的信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[14., 16.],\n",
      "        [14., 16.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# 上述方法可以实现，但是效率不高，高效的方式是使用矩阵乘法实现\n",
    "# 矩阵乘法的优势在于可以并行计算，可以大大提高运算速度。\n",
    "torch.manual_seed(42)\n",
    "\n",
    "a = torch.ones(3,3)\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "\n",
    "c = torch.matmul(a,b)\n",
    "\n",
    "print(f'a=\\n{a}')\n",
    "print(f'b=\\n{b}')\n",
    "print(f'c=\\n{c}')\n",
    "\n",
    "# 这里可以看到矩阵c中每一行各列的值都分别是矩阵b中的对应列的所有行的元素之和，这里可以看作矩阵a是一个function矩阵，这个function的功能就是将矩阵b中的每一列的所有行元素求和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[ 2.,  7.],\n",
      "        [ 8., 11.],\n",
      "        [14., 16.]])\n"
     ]
    }
   ],
   "source": [
    "# 如果对function矩阵进行变换，那么就会应用不同的计算规则对矩阵b进行计算\n",
    "\n",
    "a = torch.tril(a)\n",
    "c = torch.matmul(a,b)\n",
    "\n",
    "print(f'a=\\n{a}')\n",
    "print(f'b=\\n{b}')\n",
    "print(f'c=\\n{c}')\n",
    "\n",
    "# 现在可以看到c的每一行的对应列结果都是b的对应行加上比其行索引小的行的对应列的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=\n",
      "tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n",
      "b=\n",
      "tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "c=\n",
      "tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# 为了解决之前计算效率不足的问题，我们可以构建新的function矩阵a\n",
    "\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "sum_a = torch.sum(a,dim=1,keepdim=True)\n",
    "a = a/sum_a\n",
    "\n",
    "c = torch.matmul(a,b)\n",
    "\n",
    "print(f'a=\\n{a}')\n",
    "print(f'b=\\n{b}')\n",
    "print(f'c=\\n{c}')\n",
    "\n",
    "# 这样，无论怎么改变b的列数，都可以按照这种规则进行满足a这种function矩阵的扩展"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
